{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d741e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e41887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721794d670b14f61a21872c01951bd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Select model and load it.\n",
    "MODEL_ID = \"Menlo/Jan-nano-128k\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Select calibration dataset.\n",
    "DATASET_ID = \"mit-han-lab/pile-val-backup\"\n",
    "DATASET_SPLIT = \"validation\"\n",
    "\n",
    "# Select number of samples. 256 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 256\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "# Load dataset and preprocess.\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268d7521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a68074e5cd541f99cff38b1dc6a1919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": example[\"text\"]}],\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "ds = ds.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7056c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize inputs.\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789e16c374c54f669316da98c3a58b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-07T14:43:27.489919-0400 | reset | INFO - Compression lifecycle reset\n",
      "2025-07-07T14:43:27.494316-0400 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-07-07T14:43:27.588085-0400 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving mapping 1/4 (0 skipped): 100%|██████████| 36/36 [00:00<00:00, 774.16it/s]\n",
      "Resolving mapping 2/4 (35 skipped): 100%|██████████| 36/36 [00:00<00:00, 1291.15it/s]\n",
      "Resolving mapping 3/4 (0 skipped): 100%|██████████| 36/36 [00:00<00:00, 895.85it/s]\n",
      "Resolving mapping 4/4 (0 skipped): 100%|██████████| 36/36 [00:00<00:00, 1500.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-07T14:43:27.791949-0400 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-07-07T14:43:27.797797-0400 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing cache: 100%|██████████| 256/256 [00:00<00:00, 1129.66it/s]\n",
      "(1/37): Calibrating: 100%|██████████| 256/256 [00:04<00:00, 59.96it/s]\n",
      "Smoothing:  33%|███▎      | 1/3 [00:10<00:20, 10.35s/it]"
     ]
    }
   ],
   "source": [
    "# Configure the quantization algorithm to run.\n",
    "recipe = [\n",
    "    AWQModifier(ignore=[\"lm_head\"], scheme=\"W4A16_ASYM\", targets=[\"Linear\"]),\n",
    "]\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcd81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm generations of the quantized model look sane.\n",
    "print(\"\\n\\n\")\n",
    "print(\"========== SAMPLE GENERATION ==============\")\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b269405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-awq-asym\"\n",
    "\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wisai-modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
